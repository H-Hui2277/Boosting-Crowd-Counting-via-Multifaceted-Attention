{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import vgg_c\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\Conda\\envs\\py\\lib\\site-packages\\torch\\nn\\functional.py:3253: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = vgg_c.vgg19_trans()\n",
    "# print(model)\n",
    "i = torch.rand((1, 3, 512, 512))\n",
    "o, fetures = model(i)\n",
    "print(o.size())\n",
    "for feture in fetures:\n",
    "    print(feture.size()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.crowd import Crowd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import os\n",
    "import numpy as np\n",
    "from losses.post_prob import Post_Prob\n",
    "from losses.bay_loss import Bay_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate(batch):\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    images = torch.stack(transposed_batch[0], 0)\n",
    "    points = transposed_batch[1]  # the number of points is not fixed, keep it as a list of tensor\n",
    "    targets = transposed_batch[2]\n",
    "    st_sizes = torch.FloatTensor(transposed_batch[3])\n",
    "    return images, points, targets, st_sizes\n",
    "post_prob = Post_Prob(\n",
    "    sigma=8.0, \n",
    "    c_size=512, \n",
    "    stride=16, \n",
    "    background_ratio=0.15,\n",
    "    use_background=True,\n",
    "    device='cpu'\n",
    ")\n",
    "criterion = Bay_Loss(use_background=True, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    x : Crowd(os.path.join(\"UCF-Train-Val-Test\", x), \n",
    "              crop_size=512, \n",
    "              downsample_ratio=16, \n",
    "              is_gray=False, method=x) \n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataloaders = {\n",
    "    x : DataLoader(datasets[x], \n",
    "                   collate_fn=(train_collate if x == 'train' else default_collate),\n",
    "                   batch_size=(1 if x == 'train' else 1), \n",
    "                   shuffle=(True if x =='train' else False),\n",
    "                   num_workers=0, \n",
    "                   pin_memory=(True if x == 'train' else False))\n",
    "    for x in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Epoch\n",
    "测试各个部分数据的含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.size() = torch.Size([1, 3, 512, 512])\n",
      "points[0].size() = torch.Size([76, 2])\n",
      "st_sizes =  tensor([1580.])\n",
      "prob.size() = torch.Size([77, 1024])\n",
      "target.size() = torch.Size([76])\n",
      "target = tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7866, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5316, 0.4770, 0.8145,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5304, 0.4503,\n",
      "        0.4978, 0.3279, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        0.5221, 1.0000, 1.0000, 1.0000, 0.5685, 0.9848, 1.0000, 1.0000, 1.0000,\n",
      "        0.6059, 0.5815, 1.0000, 0.5961, 0.4556, 1.0000, 0.7179, 1.0000, 1.0000,\n",
      "        1.0000, 0.5779, 1.0000, 0.9868, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000])\n",
      "outputs.size() = torch.Size([1, 1, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\Conda\\envs\\py\\lib\\site-packages\\torch\\nn\\functional.py:3253: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "for step, (inputs, points, targets, st_sizes) in enumerate(dataloaders['train']):\n",
    "    # c, h, w = inputs.size()\n",
    "    # inputs = inputs.view(-1, c, h, w)\n",
    "    print('inputs.size() =', inputs.size())\n",
    "\n",
    "    gd_count = np.array([len(p) for p in points], dtype=np.float32)\n",
    "    points = [p for p in points]\n",
    "    targets = [t for t in targets]\n",
    "    for i, p in enumerate(points):\n",
    "        print(\"points[{}].size() = {}\".format(i, p.size()))\n",
    "\n",
    "    outputs, features = model(inputs)\n",
    "    # TODO prob_list  7\n",
    "    print('st_sizes = ', st_sizes)\n",
    "    prob_list = post_prob(points, st_sizes)\n",
    "    for prob in prob_list:\n",
    "        if prob != None:\n",
    "            print('prob.size() =', prob.size())\n",
    "        else :\n",
    "            print(\"None\")\n",
    "    for target in targets:\n",
    "        print('target.size() =',target.size())\n",
    "        print('target =', target)\n",
    "    print('outputs.size() =',outputs.size())\n",
    "    loss = criterion(prob_list, targets, outputs)\n",
    "    loss_c = 0\n",
    "    for feature in features:\n",
    "        mean_feature = torch.mean(feature, dim=0)\n",
    "        mean_sum = torch.sum(mean_feature**2)**0.5\n",
    "        cosine = 1 - torch.sum(feature*mean_feature, dim=1) / (mean_sum * torch.sum(feature**2, dim=1)**0.5 + 1e-5)\n",
    "        loss_c += torch.sum(cosine)\n",
    "    loss += loss_c\n",
    "\n",
    "    N = inputs.size(0)\n",
    "    pre_count = torch.sum(outputs.view(N, -1), dim=1).detach().cpu().numpy()\n",
    "    res = pre_count - gd_count\n",
    "    if step == 0:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0290, 0.0032, 0.4807, 0.4078, 0.8467, 0.5442, 0.1685, 0.7243, 0.6350,\n",
      "        0.0663, 0.7532, 0.5711, 0.6797, 0.7012, 0.3887, 0.1317, 0.2648, 0.2201,\n",
      "        0.9376, 0.6267])\n",
      "tensor(0.6267)\n",
      "18\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.0032, 0.0290, 0.0663, 0.1317, 0.1685, 0.2201, 0.2648, 0.3887, 0.4078,\n",
      "        0.4807, 0.5442, 0.5711, 0.6267, 0.6350, 0.6797, 0.7012, 0.7243, 0.7532]),\n",
      "indices=tensor([ 1,  0,  9, 15,  6, 17, 16, 14,  3,  2,  5, 11, 19,  8, 12, 13,  7, 10]))\n",
      "tensor([0.0032, 0.0290, 0.0663, 0.1317, 0.1685, 0.2201, 0.2648, 0.3887, 0.4078,\n",
      "        0.4807, 0.5442, 0.5711, 0.6267, 0.6350, 0.6797, 0.7012, 0.7243, 0.7532])\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "ten = torch.rand((20))\n",
    "print(ten)\n",
    "print(ten[-1])\n",
    "num = ceil(0.9 * (len(ten) - 1))\n",
    "print(num)\n",
    "topk = torch.topk(ten, num, largest=False)\n",
    "print(topk)\n",
    "print(topk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  8.,  24.,  40.,  56.,  72.,  88., 104., 120., 136., 152., 168., 184.,\n",
      "        200., 216., 232., 248., 264., 280., 296., 312., 328., 344., 360., 376.,\n",
      "        392., 408., 424., 440., 456., 472., 488., 504.])\n",
      "torch.Size([32])\n",
      "tensor([[  8.,  24.,  40.,  56.,  72.,  88., 104., 120., 136., 152., 168., 184.,\n",
      "         200., 216., 232., 248., 264., 280., 296., 312., 328., 344., 360., 376.,\n",
      "         392., 408., 424., 440., 456., 472., 488., 504.]])\n",
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "cood = torch.arange(0, 512, 16, dtype=torch.float32, device='cpu') + 16 / 2 \n",
    "print(cood)\n",
    "print(cood.size())\n",
    "cood.unsqueeze_(0)\n",
    "print(cood)\n",
    "print(cood.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_points_per_image = [9]\n",
      "all_points.size() = torch.Size([9, 2])\n",
      "9\n",
      "x.size() = torch.Size([9, 1])\n",
      "y.size() = torch.Size([9, 1])\n",
      "x_dis.size() = torch.Size([9, 32])\n",
      "y_dis.size() = torch.Size([9, 32])\n",
      "After unsqueeze...\n",
      "x_dis.size() = torch.Size([9, 1, 32])\n",
      "y_dis.size() = torch.Size([9, 32, 1])\n",
      "dis.size() = torch.Size([9, 32, 32])\n",
      "After view...\n",
      "dis.size() = torch.Size([9, 1024])\n",
      "0 dis_item.size() = torch.Size([9, 1024])\n",
      "prob.size() = torch.Size([9, 1024])\n"
     ]
    }
   ],
   "source": [
    "num_points_per_image = [len(points_per_image) for points_per_image in points]\n",
    "print('num_points_per_image =', num_points_per_image)\n",
    "all_points = torch.cat(points, dim=0)\n",
    "print('all_points.size() =', all_points.size())\n",
    "print(len(all_points))\n",
    "x = all_points[:, 0].unsqueeze_(1)\n",
    "y = all_points[:, 1].unsqueeze_(1)\n",
    "print('x.size() =', x.size())\n",
    "print('y.size() =', y.size())\n",
    "x_dis = -2 * torch.matmul(x, cood) + x * x + cood * cood\n",
    "y_dis = -2 * torch.matmul(y, cood) + y * y + cood * cood\n",
    "print(\"x_dis.size() =\", x_dis.size())\n",
    "print(\"y_dis.size() =\", y_dis.size())\n",
    "x_dis.unsqueeze_(1)\n",
    "y_dis.unsqueeze_(2)\n",
    "print(\"After unsqueeze...\")\n",
    "print(\"x_dis.size() =\", x_dis.size())\n",
    "print(\"y_dis.size() =\", y_dis.size())\n",
    "dis = y_dis + x_dis\n",
    "print(\"dis.size() =\", dis.size())\n",
    "dis = dis.view((dis.size(0), -1))\n",
    "print(\"After view...\")\n",
    "print(\"dis.size() =\", dis.size())\n",
    "dis_list = torch.split(dis, num_points_per_image)\n",
    "for i, dis_item in enumerate(dis_list):\n",
    "    print(i, \"dis_item.size() =\", dis_item.size())\n",
    "prob_list = []\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "for dis, st_size in zip(dis_list, st_sizes):\n",
    "    if len(dis) > 0:\n",
    "        min_dis = torch.clamp(torch.min(dis, dim=0, keepdim=True)[0], min=0.0)\n",
    "        d = st_size * 0.15\n",
    "        bg_dis = (d - torch.sqrt(min_dis)) ** 2\n",
    "        dis = -dis / (2.0 * 8.0 ** 2)\n",
    "        prob = softmax(dis)\n",
    "        print('prob.size() =', prob.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('action')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ec49589532473b7bb52b16885db8fbf92a14b518c0c5b08e508a9d0dcacf09a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
