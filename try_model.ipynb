{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型方法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.transformer_cosine import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from models.vgg_c import make_layers, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "nhead = 2\n",
    "num_layers = 4\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "activation = \"relu\"\n",
    "normalize_before = False\n",
    "encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                        dropout, activation, normalize_before)\n",
    "if_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "encoder = TransformerEncoder(encoder_layer, num_layers, if_norm)\n",
    "reg_layer_0 = nn.Sequential(\n",
    "    nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(128, 1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature x.size() = torch.Size([1, 512, 16, 16])\n",
      "Flatten x.size() = torch.Size([1, 512, 256])\n",
      "Transformer input x.size() = torch.Size([256, 1, 512])\n",
      "Output x.size() = torch.Size([1, 512, 16, 16])\n",
      "After upsampling x.size() = torch.Size([1, 512, 32, 32])\n",
      "Regression head x.size() = torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 512, 512))\n",
    "features_layer = make_layers(cfg['E'])\n",
    "x = features_layer(x)\n",
    "print(\"Feature x.size() =\", x.size())\n",
    "bs, c, h, w = x.shape\n",
    "x = x.flatten(2)\n",
    "print(\"Flatten x.size() =\",x.size())\n",
    "x = x.permute(2, 0, 1)\n",
    "print('Transformer input x.size() =', x.size())\n",
    "x, features = encoder(x, (h, w))\n",
    "x = x.permute(1, 2, 0).view(bs, c, h, w)\n",
    "print(\"Output x.size() =\", x.size())\n",
    "x = F.interpolate(x, size=(32, 32), mode='bilinear', align_corners=True)\n",
    "print(\"After upsampling x.size() =\", x.size())\n",
    "x = reg_layer_0(x)\n",
    "print(\"Regression head x.size() =\", x.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb8a2b2386c3df55715ecd8c8ce953e91f74eb50976ed75122b51edf92ec6bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
